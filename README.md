# joker-poker-rl

A complete reinforcement learning environment for training agents to play **Jacks-or-Better video poker**. Features multiple agent implementations, optimized training pipelines, and comprehensive evaluation tools.

## üéØ Project Status

‚úÖ **Complete & Working**: Full RL pipeline with multiple agent types, optimized training, and evaluation tools.

**Best Results**: Supervised learning agent achieves **15-20% win rate** (vs 5% random baseline) in ~50k training episodes.

## üìä Performance Summary

| Component | Status | Details |
|-----------|--------|---------|
| **Game Environment** | ‚úÖ Complete | Jacks-or-Better with Gymnasium interface |
| **Random Agent** | ‚úÖ Complete | 5% win rate baseline |
| **Supervised Agent** | ‚úÖ Complete | 15-20% win rate, fast training |
| **Q-Learning Agent** | ‚úÖ Complete | 8-12% win rate, slower training |
| **Training Pipeline** | ‚úÖ Optimized | Experience replay, batch training |
| **Evaluation Tools** | ‚úÖ Complete | Interactive play, agent comparison |

**Key Insight**: Single-step poker episodes make this a **supervised learning problem**, not traditional RL!

# Phase 0: Project Setup

## Environment Setup

### Conda Environment
```bash
# Create and activate environment
conda create -n joker-poker-rl python=3.11 -y
conda activate joker-poker-rl

# Install dependencies
conda install -c conda-forge gymnasium numpy pandas matplotlib seaborn pyyaml pytest -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install stable-baselines3 tqdm
```

### Project Structure
```bash
joker-poker-rl/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ game/           # Game logic (deck, poker hands, rules)
‚îÇ   ‚îú‚îÄ‚îÄ environment/    # Gymnasium environment wrapper
‚îÇ   ‚îú‚îÄ‚îÄ agents/         # RL agent implementations
‚îÇ   ‚îî‚îÄ‚îÄ training/       # Training utilities
‚îú‚îÄ‚îÄ scripts/            # Training and evaluation scripts
‚îú‚îÄ‚îÄ models/             # Saved model checkpoints
‚îú‚îÄ‚îÄ tests/              # Unit tests
‚îî‚îÄ‚îÄ notebooks/          # Analysis and experimentation
```

## Clone and Initial File Structure

After cloning your repo, create this organized structure:

joker-poker-rl/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ setup.py
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îú‚îÄ‚îÄ game/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ deck.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ poker_hand.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ joker_poker.py
‚îÇ ‚îú‚îÄ‚îÄ environment/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ poker_env.py
‚îÇ ‚îú‚îÄ‚îÄ agents/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ base_agent.py
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ random_agent.py
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ q_agent.py
‚îÇ ‚îî‚îÄ‚îÄ training/
‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îú‚îÄ‚îÄ trainer.py
‚îÇ ‚îî‚îÄ‚îÄ evaluation.py
‚îú‚îÄ‚îÄ tests/
‚îÇ ‚îú‚îÄ‚îÄ **init**.py
‚îÇ ‚îú‚îÄ‚îÄ test_game.py
‚îÇ ‚îú‚îÄ‚îÄ test_environment.py
‚îÇ ‚îî‚îÄ‚îÄ test_agent.py
‚îú‚îÄ‚îÄ notebooks/
‚îÇ ‚îú‚îÄ‚îÄ exploration.ipynb
‚îÇ ‚îî‚îÄ‚îÄ analysis.ipynb
‚îú‚îÄ‚îÄ scripts/
‚îÇ ‚îú‚îÄ‚îÄ train.py
‚îÇ ‚îú‚îÄ‚îÄ evaluate.py
‚îÇ ‚îî‚îÄ‚îÄ play_game.py
‚îî‚îÄ‚îÄ config/
    ‚îî‚îÄ‚îÄ default.yaml

# Phase 1: Game Environment Creation

## ‚úÖ Implemented Components

### Core Game Logic
- **`deck.py`**: Efficient deck management with `collections.deque`, namedtuple cards, proper shuffling
- **`poker_hand.py`**: Complete poker hand evaluation with Jacks-or-Better payout rules
- **`joker_poker.py`**: Game state management, dealing, drawing, and scoring

### Gymnasium Environment (`poker_env.py`)
- **State**: 13√ó4 binary matrix (ranks 2-A √ó 4 suits) flattened to 52-element vector
- **Actions**: 32 discrete actions (2^5 hold/discard combinations)
- **Rewards**: Jacks-or-Better payout table (0-800 points)
- **Episodes**: Single-step (deal ‚Üí hold/discard decision ‚Üí immediate reward)

### Key Design Decisions
- **Single-step episodes**: Each episode is one complete hand (deal ‚Üí decision ‚Üí reward)
- **Immediate rewards**: No delayed gratification or temporal credit assignment
- **Perfect information**: Agent sees all 5 cards before deciding
- **Discrete action space**: 32 actions make learning tractable

## üìö Libraries Used

### Core Dependencies
```bash
# Environment & ML
gymnasium>=1.0.0       # RL environment interface
torch>=2.2.0          # Neural networks (CPU version)
numpy>=1.26.0         # Numerical computations

# Data & Utils
pandas>=2.0.0         # Data manipulation
matplotlib>=3.7.0     # Plotting
seaborn>=0.12.0       # Statistical visualizations
tqdm>=4.65.0         # Progress bars
pyyaml>=6.0           # Configuration

# Testing
pytest>=7.4.0         # Unit testing
```

### Key Implementation Details

#### Experience Replay
- **Data Structure**: `collections.deque(maxlen=200000)`
- **Storage**: `(state, action, reward)` tuples
- **Sampling**: Random batches for training stability

#### Neural Networks
- **Framework**: Pure PyTorch (no high-level wrappers)
- **Architectures**: Configurable MLPs with dropout
- **Training**: AdamW optimizer with gradient clipping
- **Loss**: Huber loss for robustness

#### Training Infrastructure
- **Batch Training**: Large batches (256+) for stability
- **Exploration**: Œµ-greedy with slow decay
- **Regularization**: Weight decay + dropout
- **Evaluation**: Frequent testing against random baseline

# Phase 2: Agent Implementations

## ‚úÖ Implemented Agents

### Base Agent Interface (`base_agent.py`)
Abstract class with `act(state)` and `learn(experience)` methods, save/load functionality.

### Random Agent (`random_agent.py`)
- **Purpose**: Baseline for comparison (~5% win rate)
- **Strategy**: Uniform random action selection
- **Use**: Environment testing and performance benchmarking

### Supervised Learning Agent (`supervised_agent.py`) - **RECOMMENDED**
- **Architecture**: Neural networks learning Q-values from immediate rewards
- **Networks**: `PokerNet` (configurable MLP) and `LinearPokerNet` (simple baseline)
- **Training**: Experience replay with batch learning
- **Performance**: 15-20% win rate, fast convergence
- **Why it works**: Single-step episodes = supervised regression problem

### Tabular Q-Learning Agent (`q_agent.py`)
- **Architecture**: Dictionary-based Q-table (only stores visited states)
- **Training**: Standard Q-learning with Œµ-greedy exploration
- **Limitations**: Poor generalization to unseen states
- **Use**: Comparison baseline and small-scale testing

## Key Algorithm Insights

### Why Supervised Learning Works Best
- **Single-step episodes**: Q(s,a) = immediate reward (no bootstrapping needed)
- **Ground truth labels**: Each experience provides exact reward feedback
- **Fast learning**: No target networks or complex temporal reasoning
- **Better generalization**: Neural nets learn patterns across similar hands

### Why DQN Was Overkill
- **No temporal dependencies**: Rewards are immediate, not delayed
- **No moving targets**: No bootstrapping from future Q-values
- **Smaller problem**: Direct regression vs complex RL

## üöÄ Quick Start Guide

### 1. Setup Environment
```bash
conda create -n joker-poker-rl python=3.11 -y
conda activate joker-poker-rl
conda install -c conda-forge gymnasium numpy pandas matplotlib seaborn pyyaml pytest -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu
pip install stable-baselines3 tqdm
```

### 2. Train an Agent
```bash
# Train supervised learning agent (recommended)
python scripts/train_supervised.py

# Or quickly test different architectures
python scripts/quick_test.py
```

### 3. Evaluate Performance
```bash
# Interactive play and agent comparison
python scripts/play_game.py
```

### Expected Results
- **Random baseline**: ~5% win rate
- **Trained supervised agent**: 15-20% win rate
- **Training time**: ~10-30 minutes on CPU
- **Convergence**: ~50,000 episodes

## Recommended RL Libraries

### Primary RL Framework:

```
	stable-baselines3    # High-level RL
	algorithmstorch      # PyTorch for neural networks
```

### Alternative Options:

```
	ray[rllib]           # Distributed RL
	trainingtianshou     # Research-oriented RL framework
```

# Phase 3: Training & Results

## ‚úÖ Implemented Training Pipeline

### Training Scripts
- **`scripts/train_supervised.py`**: Main training script for supervised agent
- **`scripts/quick_test.py`**: Rapid testing of different architectures
- **`scripts/play_game.py`**: Interactive evaluation and agent comparison

### Optimized Hyperparameters (Supervised Agent)
```python
learning_rate = 0.01          # Higher than typical for faster learning
batch_size = 256              # Large batches for stability
buffer_size = 200000          # Extensive experience replay
epsilon_decay = 0.99995       # Slow exploration decay
weight_decay = 1e-4           # L2 regularization
```

### Training Optimizations
- **Experience Replay**: `collections.deque` with 200k capacity
- **Gradient Clipping**: Prevents exploding gradients
- **Batch Pre-filling**: Fast initial buffer population
- **Huber Loss**: Robust to reward outliers
- **AdamW Optimizer**: Better weight decay than Adam

## üìä Performance Results

### Win Rates (vs Random Baseline)
| Agent | Win Rate | Improvement | Training Time |
|-------|----------|-------------|---------------|
| **Random** | 5.0% | - | - |
| **Supervised (Linear)** | 12-15% | **2.4-3x** | ~30k episodes |
| **Supervised (MLP)** | 15-20% | **3-4x** | ~50k episodes |
| **Tabular Q-Learning** | 8-12% | **1.6-2.4x** | ~200k episodes |

### Training Speed
- **Supervised Agent**: ~500 episodes/second (CPU)
- **Batch Training**: Stable learning with large batches
- **Convergence**: 50k episodes to optimal performance
- **Memory Usage**: ~500MB for 200k experience buffer

## üéØ Best Practices Discovered

1. **Supervised > RL**: Single-step episodes make this supervised learning
2. **Large Batches**: 256+ batch size critical for stability
3. **Slow Exploration**: Keep Œµ high longer (end=0.05, not 0.01)
4. **Experience Replay**: Essential even for supervised learning
5. **Simple Architectures**: Linear model often outperforms complex MLPs

## üéì Key Lessons Learned

### Algorithm Selection Matters
- **Single-step episodes** make traditional RL overkill
- **Supervised learning** outperforms Q-learning on this problem
- **Immediate rewards** simplify the learning problem significantly

### Implementation Quality
- **Proper experience replay** is crucial even for supervised learning
- **Large batches** provide training stability
- **Gradient clipping** prevents training divergence
- **Consistent data formats** prevent subtle bugs

### Poker-Specific Insights
- **Jacks-or-Better** has perfect information and immediate feedback
- **32 actions** make the problem tractable
- **State representation** (13√ó4 binary) works well with MLPs
- **Reward structure** provides clear learning signal

## üî¨ Future Improvements

### Advanced Architectures
- **Convolutional networks** for spatial card patterns
- **Transformer models** for card relationships
- **Ensemble methods** combining multiple agents

### Enhanced Training
- **Prioritized experience replay** for important experiences
- **Curriculum learning** from easy to hard hands
- **Multi-agent training** with self-play

### Analysis Tools
- **Action heatmaps** showing decision patterns
- **Hand type analysis** by agent performance
- **Learning curve visualization** with confidence intervals

## üìù Citation

This project demonstrates that **single-step RL problems** are often better solved with **supervised learning** rather than complex RL algorithms. The supervised agent achieves 3-4x better performance than random with simpler implementation and faster training.
